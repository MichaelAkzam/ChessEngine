{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4PVknJlquSct"
      },
      "outputs": [],
      "source": [
        "pip install chess\n",
        "pip install scikit-learn\n",
        "pip install tensorflow\n",
        "pip install python-chess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JoWIWvxs-WD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "import chess.pgn\n",
        "import numpy as np\n",
        "from tensorflow import python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math  # Import the math module for NaN checking\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "is_mps_available = torch.backends.mps.is_available()\n",
        "device = torch.device(\"mps\" if is_mps_available else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmkuhU0ts-WD"
      },
      "outputs": [],
      "source": [
        "# raw source pgn file\n",
        "data_file = 'aug2023_data_2gb.pgn'\n",
        "\n",
        "#data_file = '500mb_data.pgn'\n",
        "\n",
        "# pgn after extracting k games that stockfish evals >= 2000\n",
        "destination_pgn = 'k_games_with_eval.pgn'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baxoxiJK8zBq"
      },
      "outputs": [],
      "source": [
        "# Open the original PGN file to read from\n",
        "with open(data_file, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Regular expression to match individual games\n",
        "games = re.split(r'\\n\\n(?=\\[Event)', content)\n",
        "\n",
        "# Function to check if a player in the game has ELO rating >= 1600\n",
        "def has_high_elo(game_text):\n",
        "    #white_elo = int(re.search(r'\\[WhiteElo \"(\\d+)\"\\]', game_text).group(1))\n",
        "    #black_elo = int(re.search(r'\\[BlackElo \"(\\d+)\"\\]', game_text).group(1))\n",
        "    return True\n",
        "\n",
        "# Function to check if a game has evaluation annotations\n",
        "def has_eval_annotations(game_text):\n",
        "    return bool(re.search(r'\\{\\s*\\[%eval [^\\}]*\\]\\s*\\}', game_text))\n",
        "\n",
        "# Filter games that have eval annotations and at least one player with ELO >= 1600\n",
        "eval_games = [game for game in games if has_eval_annotations(game) and has_high_elo(game)]\n",
        "\n",
        "# Print the number of games with evaluations\n",
        "print(f\"Extracted {len(eval_games)} games with evaluation annotations.\")\n",
        "\n",
        "i = 0\n",
        "k = 1000\n",
        "\n",
        "with open(destination_pgn, 'w') as file:\n",
        "  for game in eval_games:\n",
        "      i = i + 1\n",
        "      file.write(game + \"\\n\")\n",
        "      if i >= k:\n",
        "        break\n",
        "\n",
        "print(f\"Saved {i} games.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePqvWgsffEno"
      },
      "outputs": [],
      "source": [
        "pgn_file_path = destination_pgn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMQBGqv1e9Tp"
      },
      "outputs": [],
      "source": [
        "# add empty lines between games in pgn file to fix extraction issues\n",
        "\n",
        "def add_empty_line_between_games(pgn_file_path):\n",
        "    # Read the entire PGN file\n",
        "    with open(pgn_file_path, 'r') as file:\n",
        "        pgn_content = file.read()\n",
        "\n",
        "    # Replace occurrences of \"1-0\", \"0-1\", or \"1/2-1/2\" followed by \"[Event\" with the same string plus an empty line\n",
        "    pgn_content = pgn_content.replace(\"1-0\\n[Event\", \"1-0\\n\\n[Event\")\n",
        "    pgn_content = pgn_content.replace(\"0-1\\n[Event\", \"0-1\\n\\n[Event\")\n",
        "    pgn_content = pgn_content.replace(\"1/2-1/2\\n[Event\", \"1/2-1/2\\n\\n[Event\")\n",
        "\n",
        "    # Write the modified content back to the file\n",
        "    with open(pgn_file_path, 'w') as file:\n",
        "        file.write(pgn_content)\n",
        "\n",
        "\n",
        "add_empty_line_between_games(pgn_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPTaBBp4uHwV"
      },
      "outputs": [],
      "source": [
        "# write FEN and evals to .csv file\n",
        "def write_fens_and_evals_directly_to_file(pgn_file_path, output_file_path):\n",
        "    game_num = 0\n",
        "\n",
        "    with open(pgn_file_path) as pgn, open(output_file_path, 'w') as output_file:\n",
        "        while True:\n",
        "            game = chess.pgn.read_game(pgn)\n",
        "            if game is None:\n",
        "                break  # No more games in the PGN file\n",
        "\n",
        "            game_num += 1\n",
        "            board = game.board()\n",
        "\n",
        "            for node in game.mainline():\n",
        "                move = node.move\n",
        "                board.push(move)\n",
        "                fen = board.fen()\n",
        "\n",
        "                # Extract evaluation from the comment if it exists\n",
        "                eval_comment = node.comment\n",
        "                eval_score = None\n",
        "                if \"[%eval\" in eval_comment:\n",
        "                    try:\n",
        "                        eval_score = eval_comment.split(\"[%eval \")[1].split(\"]\")[0].strip()\n",
        "                        if not eval_score.startswith(\"#\"):\n",
        "                            eval_score = float(eval_score)\n",
        "                    except (IndexError, ValueError):\n",
        "                        eval_score = None\n",
        "\n",
        "                # Write the FEN and evaluation to the file\n",
        "                output_file.write(f\"{fen},{eval_score}\\n\")\n",
        "\n",
        "# read FEN and evals from .csv file\n",
        "def read_fens_and_evals_from_file(file_path):\n",
        "    all_fens_and_evals = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            fen, eval_str = line.strip().split(',')\n",
        "            eval_score = float(eval_str) if eval_str.replace('.', '', 1).isdigit() else None\n",
        "            all_fens_and_evals.append((fen, eval_score))\n",
        "    return all_fens_and_evals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA8YNuRus-WF"
      },
      "outputs": [],
      "source": [
        "# write FEN and evals per move for each game to a .csv file\n",
        "\n",
        "output_file_path = 'output_file.csv'\n",
        "\n",
        "#write_fens_and_evals_directly_to_file(pgn_file_path, output_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOrV9Fuls-WF"
      },
      "outputs": [],
      "source": [
        "# Call the function to read data from the file\n",
        "\n",
        "all_fens_and_evals_extracted = read_fens_and_evals_from_file(output_file_path)\n",
        "print(len(all_fens_and_evals_extracted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2Y_H46vkSLi"
      },
      "outputs": [],
      "source": [
        "### Function to convert FENs to one hot encoding\n",
        "\n",
        "# Define the order of pieces for one-hot encoding\n",
        "piece_order = 'PNBRQKpnbrqk'\n",
        "piece_to_index = {piece: index for index, piece in enumerate(piece_order)}\n",
        "\n",
        "def fen_to_one_hot(fen):\n",
        "    # Split the FEN string into its components\n",
        "    parts = fen.split(' ')\n",
        "    rows = parts[0].split('/')\n",
        "\n",
        "    # Initialize the board to be 8x8x12 for one-hot encoded pieces\n",
        "    one_hot_board = np.zeros((8, 8, len(piece_order)), dtype=np.float32)\n",
        "\n",
        "    for row_index, row in enumerate(rows):\n",
        "        col_index = 0\n",
        "        for char in row:\n",
        "            if char.isdigit():\n",
        "                # Empty squares are skipped\n",
        "                col_index += int(char)\n",
        "            else:\n",
        "                # Set the one-hot encoding for the piece at the appropriate location\n",
        "                one_hot_board[row_index, col_index, piece_to_index[char]] = 1\n",
        "                col_index += 1\n",
        "\n",
        "    # Flatten the one-hot encoded board to a single vector\n",
        "    flat_board = one_hot_board.flatten()\n",
        "\n",
        "    # Encode the active color ('w' -> 1, 'b' -> 0)\n",
        "    active_color = 1 if parts[1] == 'w' else 0\n",
        "\n",
        "    # Encode castling availability\n",
        "    castling = [parts[2].count('K'), parts[2].count('Q'), parts[2].count('k'), parts[2].count('q')]\n",
        "\n",
        "    # Encode the en passant target square\n",
        "    en_passant = np.zeros((8, 8), dtype=np.float32)\n",
        "    if parts[3] != '-':\n",
        "        col = ord(parts[3][0]) - ord('a')\n",
        "        row = 8 - int(parts[3][1])\n",
        "        en_passant[row, col] = 1\n",
        "\n",
        "    # Flatten the en passant board to a single vector\n",
        "    flat_en_passant = en_passant.flatten()\n",
        "\n",
        "    # Encode the halfmove clock and fullmove number\n",
        "    halfmove_clock = int(parts[4])\n",
        "    fullmove_number = int(parts[5])\n",
        "\n",
        "    # Combine all parts into a single input vector\n",
        "    input_vector = np.concatenate([flat_board, [active_color], castling, flat_en_passant, [halfmove_clock, fullmove_number]])\n",
        "\n",
        "    return input_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RcTNTP6NMNd5"
      },
      "outputs": [],
      "source": [
        "# Convert all FENs to one hot encoding, store one-hot and evals in parallel lists to be fed into model\n",
        "\n",
        "one_hot_board_representations = []\n",
        "stockfish_evaluations = []\n",
        "\n",
        "# Iterate over the first 200 tuples in all_fens_and_evals\n",
        "for i in range(len(all_fens_and_evals_extracted)):\n",
        "    # Extract the FEN and eval from the current tuple\n",
        "    fen, eval = all_fens_and_evals_extracted[i]\n",
        "\n",
        "    # Convert FEN to one-hot representation\n",
        "    one_hot = fen_to_one_hot(fen)\n",
        "\n",
        "    # Get evaluation from Stockfish\n",
        "    #board = chess.Board(fen)\n",
        "    #info = engine.analyse(board, chess.engine.Limit(time=0.1))\n",
        "\n",
        "    # Check if the evaluation is not None and is a valid number (not NaN)\n",
        "    #evaluation = info[\"score\"].relative.score()\n",
        "    evaluation = eval\n",
        "\n",
        "    #print(evaluation)\n",
        "\n",
        "    if str(evaluation)[0] == '#':\n",
        "      if str(evaluation[1] == '-'):\n",
        "        evaluation = -15\n",
        "      else:\n",
        "        evaluation = 15\n",
        "\n",
        "    if evaluation:\n",
        "      if evaluation > 15:\n",
        "        evaluation = 15\n",
        "      elif evaluation < -15:\n",
        "        evaluation = -15\n",
        "\n",
        "    if evaluation is not None and not math.isnan(evaluation):\n",
        "        # Add the one-hot representation and evaluation\n",
        "        one_hot_board_representations.append(one_hot)\n",
        "        stockfish_evaluations.append(evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5cQDkBbs-WG"
      },
      "outputs": [],
      "source": [
        "# Create dummy data for testing\n",
        "num_samples = 100  # Set the number of samples\n",
        "#X = np.random.randint(2, size=(num_samples, 839))  # 839 features (one-hot encoded)\n",
        "#y = np.random.randn(num_samples)  # Corresponding evaluations\n",
        "\n",
        "# Assuming you have a dataset with input arrays and corresponding evaluations\n",
        "# Replace this with your actual data\n",
        "X = one_hot_board_representations  # Sample input data as a NumPy array\n",
        "y = stockfish_evaluations          # Sample evaluation data as a NumPy array\n",
        "\n",
        "device = torch.device(\"mps\" if is_mps_available else \"cpu\")\n",
        "is_mps_available = torch.backends.mps.is_available()\n",
        "print(\"Is Apple GPU available:\", is_mps_available)\n",
        "\n",
        "\n",
        "# Convert your data to NumPy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y, dtype=np.float32)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the enhanced neural network model with more layers\n",
        "class ANNModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(ANNModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.bn4 = nn.BatchNorm1d(64)\n",
        "        self.dropout4 = nn.Dropout(0.5)\n",
        "        self.fc5 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.bn2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.bn3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = self.bn4(x)\n",
        "        x = self.dropout4(x)\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Check for GPU availability and use it if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ANNModel(839).to(device)\n",
        "\n",
        "# Function to train the PyTorch model\n",
        "def train_model(model, train_input, train_target, val_input, val_target, num_epochs=10, learning_rate=0.005, batch_size=32):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train_size = len(train_input)\n",
        "\n",
        "    # Lists for storing loss values\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_loss_accum = 0\n",
        "\n",
        "        for batch_start in range(0, train_size, batch_size):\n",
        "            batch_end = min(batch_start + batch_size, train_size)\n",
        "            train_inputs = torch.from_numpy(train_input[batch_start:batch_end]).float().to(device)\n",
        "            train_targets = torch.from_numpy(train_target[batch_start:batch_end]).float().view(-1, 1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            train_outputs = model(train_inputs)\n",
        "            train_loss = criterion(train_outputs, train_targets)\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss_accum += train_loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss_accum / (train_size // batch_size)\n",
        "        training_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_inputs = torch.from_numpy(val_input).float().to(device)\n",
        "            val_targets = torch.from_numpy(val_target).float().view(-1, 1).to(device)\n",
        "            val_outputs = model(val_inputs)\n",
        "            val_loss = criterion(val_outputs, val_targets)\n",
        "            validation_losses.append(val_loss.item())\n",
        "\n",
        "        print(f'Training Loss: {avg_train_loss}  Validation Loss: {val_loss.item()}')\n",
        "\n",
        "    # Plot the training and validation loss\n",
        "    plt.plot(training_losses, label='Training Loss')\n",
        "    plt.plot(validation_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Train the enhanced PyTorch model and plot losses\n",
        "train_model(model, X_train, y_train, X_test, y_test, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX2CAbOWs-WH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Function to evaluate the model on test data\n",
        "def evaluate_model(model, test_input, test_target):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_inputs = torch.from_numpy(test_input).float().to(device)\n",
        "        test_targets = torch.from_numpy(test_target).float().view(-1, 1).to(device)\n",
        "        predictions = model(test_inputs)\n",
        "        predictions = predictions.cpu().numpy()\n",
        "\n",
        "    mse = mean_squared_error(test_target, predictions)\n",
        "    mae = mean_absolute_error(test_target, predictions)\n",
        "    r2 = r2_score(test_target, predictions)\n",
        "\n",
        "    print(f\"Test MSE: {mse}\")\n",
        "    print(f\"Test MAE: {mae}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}